{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6448a60a",
   "metadata": {},
   "source": [
    "# CS103 Project - Logistic Regression\n",
    "\n",
    "The logistic regression model is a statistical model used to predict a **binary outcome**. It is a statistical method used to model the relationship between one or more independent variables and a binary dependent variable — that is, an outcome that has two possible values, such as \"yes/no\" or \"success/failure.\" Unlike linear regression, which predicts a continuous value, logistic regression predicts the probability that a given input belongs to a particular category. It does this by applying the logistic (sigmoid) function to transform the output of a linear combination of the input variables into a value between 0 and 1, representing a probability. The sigmoid function is given by:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "This makes logistic regression widely used in fields like medicine, finance, and machine learning for classification problems such as disease prediction, spam detection, and customer churn analysis. Below is the comparison between linear regression and logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05992298",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.5' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Define all imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f88e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)  # feature values\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)  # target values with some noise\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, color='blue', label='Data Points')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(X, y_pred, color='red', linewidth=2, label='Regression Line')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29dbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid helper function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values for the curve\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y = sigmoid(x)\n",
    "\n",
    "# Generate sample points concentrated near 0 and near 1\n",
    "x_near_0 = np.random.uniform(-10, -5, 15)  # points where sigmoid ~ 0\n",
    "x_middle = np.random.uniform(-2, 2, 10)    # points where sigmoid ~ 0.5\n",
    "x_near_1 = np.random.uniform(5, 10, 15)    # points where sigmoid ~ 1\n",
    "\n",
    "# Compute sigmoid values for these points\n",
    "y_near_0 = sigmoid(x_near_0)\n",
    "y_middle = sigmoid(x_middle)\n",
    "y_near_1 = sigmoid(x_near_1)\n",
    "\n",
    "# Plot the sigmoid curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, color='blue', linewidth=2)\n",
    "\n",
    "# Plot sample points\n",
    "plt.scatter(x_near_0, y_near_0, color='red', zorder=5)\n",
    "plt.scatter(x_near_1, y_near_1, color='orange', zorder=5)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('σ(x)')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bf281",
   "metadata": {},
   "source": [
    " The sigmoid function is primarily used to map any real-valued number into a probability-like value between 0 and 1, making it particularly useful in fields like statistics, machine learning, and neural networks. Its smooth, S-shaped curve ensures that extreme negative inputs produce outputs close to 0, extreme positive inputs produce outputs close to 1, and values near zero produce outputs near 0.5. This property allows the sigmoid function to act as a **squashing function**, compressing unbounded input values into a finite range, which is essential for modeling probabilities and making binary classifications. In logistic regression, the sigmoid function converts the linear combination of input features into a probability that an event belongs to a particular class, while in neural networks, it helps introduce non-linearity, enabling the model to capture complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb55d7",
   "metadata": {},
   "source": [
    "## Connection to Linear Algebra\n",
    "\n",
    "1. **Data Representation as Matrices**  \n",
    "   Represent the inputs as a matrix $X$ of size $n \\times m$, where $n$ is the number of samples and $m$ is the number of features.  \n",
    "   \n",
    "   $$ \n",
    "   X = \n",
    "   \\begin{bmatrix}\n",
    "   x_{11} & x_{12} & \\dots & x_{1m} \\\\\n",
    "   x_{21} & x_{22} & \\dots & x_{2m} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   x_{n1} & x_{n2} & \\dots & x_{nm}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   \n",
    "   Weights are represented as a vector $w$ of size $m \\times 1$:\n",
    "   \n",
    "   $$w = \n",
    "   \\begin{bmatrix}\n",
    "   w_1 \\\\\n",
    "   w_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   w_m\n",
    "   \\end{bmatrix}$$\n",
    "\n",
    "2. **Linear Combination in Matrix Form**  \n",
    "   Compute the linear combination of inputs and weights using matrix multiplication:\n",
    "   \n",
    "   $$z = X w$$\n",
    "   \n",
    "   Here, $z$ is an $n \\times 1$ vector where each entry corresponds to a sample:  \n",
    "   \n",
    "   $$z_i = w_1 x_{i1} + w_2 x_{i2} + \\dots + w_m x_{im}$$\n",
    "\n",
    "3. **Gradient Descent Using Matrices**  \n",
    "   The gradient of the binary cross-entropy loss can be computed efficiently using matrices:\n",
    "   \n",
    "   $$\\nabla_w L = \\frac{1}{n} X^T (\\hat{y} - y)$$\n",
    "   \n",
    "   This allows vectorized updates of the weight vector $w$ during gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd8c7e5",
   "metadata": {},
   "source": [
    "## Example: Logistic Regression Example: Predicting Purchase of a Premium Online Subscription\n",
    "\n",
    "### Explanation\n",
    "\n",
    "This example simulates a **logistic regression scenario** where we are trying to predict whether a person will **purchase a premium online subscription service** (such as a fitness app, streaming platform, or software tool) based on multiple independent variables:\n",
    "\n",
    "- **Age** (in years)  \n",
    "- **Annual Income** (in dollars)  \n",
    "- **Years of Professional Experience**  \n",
    "\n",
    "Each individual's data is represented as a **row in a matrix $X$**, with an additional column of 1s to account for the bias term. The **weights vector $w$** encodes how strongly each feature influences the likelihood of purchasing the subscription. By computing the **linear combination**:\n",
    "\n",
    "$$z = X w$$\n",
    "\n",
    "we obtain a score for each individual. This score is then passed through the **sigmoid function** to map it into a probability between 0 and 1:\n",
    "\n",
    "$$\\hat{y}_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}}$$\n",
    "\n",
    "Finally, the **binary outcome** $y_i$ is generated based on this probability, simulating whether a person actually purchases the subscription:\n",
    "\n",
    "$$y_i \\sim \\text{Bernoulli}(\\hat{y}_i)$$\n",
    "\n",
    "The dataset can be displayed in a table to show all inputs, the linear combination $z$, predicted probabilities, and the actual binary outcome. This helps visualize how logistic regression transforms raw features into predictions for real-world decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### Matrix Representation\n",
    "\n",
    "Let:\n",
    "\n",
    "$$X =\n",
    "\\begin{bmatrix}\n",
    "1 & \\text{age}_1 & \\text{income}_1 & \\text{experience}_1 \\\\\n",
    "1 & \\text{age}_2 & \\text{income}_2 & \\text{experience}_2 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & \\text{age}_n & \\text{income}_n & \\text{experience}_n\n",
    "\\end{bmatrix}_{n \\times 4}, \n",
    "\\quad\n",
    "w =\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\ w_{\\text{age}} \\\\ w_{\\text{income}} \\\\ w_{\\text{experience}}\n",
    "\\end{bmatrix}_{4 \\times 1}$$\n",
    "\n",
    "1. **Linear combination (score for each individual):**\n",
    "\n",
    "$$z_i = w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i + w_{\\text{experience}} \\cdot \\text{experience}_i$$\n",
    "\n",
    "2. **Predicted probability of purchasing the premium subscription:**\n",
    "\n",
    "$$\\hat{y}_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}}$$\n",
    "\n",
    "3. **Binary outcome (purchase or not):**\n",
    "\n",
    "$$y_i \\sim \\text{Bernoulli}(\\hat{y}_i)$$\n",
    "\n",
    "Where $y_i = 1$ indicates the person purchased the subscription, and $y_i = 0$ indicates they did not.\n",
    "\n",
    "---\n",
    "\n",
    "This Markdown cell now fully explains the example, the real-life context, and the matrix formulation used in logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data to create a clear S-curve pattern with variable income and experience\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "# Create age values that span across the sigmoid transition zone\n",
    "# This ensures we get points both near 0 and near 1, with transition in between\n",
    "age_low = np.random.uniform(20, 35, 15)     # Ages that will likely give low probabilities\n",
    "age_mid = np.random.uniform(35, 50, 20)     # Ages in transition zone\n",
    "age_high = np.random.uniform(50, 65, 15)    # Ages that will likely give high probabilities\n",
    "age = np.concatenate([age_low, age_mid, age_high])\n",
    "\n",
    "# Variable income and experience - but with controlled ranges to maintain S-curve\n",
    "income = np.random.uniform(30000, 80000, n_samples)     # Variable income\n",
    "experience = np.random.uniform(5, 30, n_samples)        # Variable experience\n",
    "\n",
    "# Weights designed to create clear S-curve with age as PRIMARY factor\n",
    "# Income and experience have smaller coefficients to add variation without disrupting S-curve\n",
    "w0 = -12        # bias - keeps the curve centered\n",
    "w_age = 0.25    # STRONG age coefficient (main driver of S-curve)\n",
    "w_income = 0.00002  # small income effect (adds some variation)\n",
    "w_experience = 0.03  # small experience effect (adds some variation)\n",
    "w = np.array([w0, w_age, w_income, w_experience]).reshape(-1,1)\n",
    "\n",
    "# Represent it as a matrix\n",
    "X = np.column_stack((np.ones(n_samples), age, income, experience)) # n x (m+1)\n",
    "\n",
    "# Compute linear combination and predicted probabilities\n",
    "z_matrix = X @ w\n",
    "y_prob = sigmoid(z_matrix)\n",
    "\n",
    "# Generate binary dependent variable using logistic decision rule\n",
    "y = (y_prob.flatten() >= 0.5).astype(int)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Income': income,\n",
    "    'Experience': experience,\n",
    "    'Linear Combination (z)': z_matrix.flatten(),\n",
    "    'Predicted Probability': y_prob.flatten(),\n",
    "    'Purchased (y)': y\n",
    "})\n",
    "\n",
    "# Sort by age for better visualization\n",
    "df = df.sort_values('Age').reset_index(drop=True)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the S-curve plot with variable income and experience\n",
    "# Use mean values for the main curve, but show actual data points with their real values\n",
    "income_mean = df['Income'].mean()\n",
    "experience_mean = df['Experience'].mean()\n",
    "\n",
    "# Create a wide age range to show the full S-curve (using mean income/experience)\n",
    "age_range = np.linspace(15, 70, 300)  # More points for smoother curve\n",
    "X_plot = np.column_stack((\n",
    "    np.ones(300),\n",
    "    age_range,\n",
    "    np.full(300, income_mean),\n",
    "    np.full(300, experience_mean)\n",
    "))\n",
    "\n",
    "# Use the same weights as in data generation\n",
    "w_vector = np.array([w0, w_age, w_income, w_experience]).reshape(-1,1)\n",
    "z_plot = X_plot @ w_vector\n",
    "prob_plot = sigmoid(z_plot)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the smooth S-curve (based on mean income/experience)\n",
    "plt.plot(age_range, prob_plot, color='blue', linewidth=3, \n",
    "         label=f'Sigmoid Curve (Income={income_mean:.0f}, Experience={experience_mean:.1f})')\n",
    "\n",
    "# Plot actual data points with their individual probabilities\n",
    "# Color points by their actual outcome, but position them at their actual probability\n",
    "colors = ['red' if y_val == 0 else 'orange' for y_val in df['Purchased (y)']]\n",
    "scatter = plt.scatter(df['Age'], df['Predicted Probability'], c=colors, s=80, alpha=0.8, \n",
    "                     edgecolors='black', linewidth=0.5, \n",
    "                     label='Actual Data Points (with variable Income/Experience)')\n",
    "\n",
    "# Add a horizontal line at 0.5 to show the decision boundary\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, linewidth=2, \n",
    "           label='Decision Boundary (0.5)')\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('Age (with variable Income and Experience)', fontsize=14)\n",
    "plt.ylabel('Predicted Probability', fontsize=14)\n",
    "plt.title('Logistic Regression: S-Curve with Variable Features', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10, loc='center left')\n",
    "plt.ylim(-0.15, 1.15)\n",
    "plt.xlim(15, 70)\n",
    "\n",
    "# Add text annotations\n",
    "plt.text(25, 0.85, 'S-shaped Sigmoid Curve\\n(Age is primary factor)', fontsize=11, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "plt.text(45, 0.15, f'Transition around age ≈ {-w0/w_age:.0f}\\n(±variation from Income/Experience)', \n",
    "         fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed summary statistics\n",
    "print(f\"=== LOGISTIC REGRESSION SUMMARY ===\")\n",
    "print(f\"Age coefficient: {w_age:.3f} (primary driver)\")\n",
    "print(f\"Income coefficient: {w_income:.6f} (adds variation)\")\n",
    "print(f\"Experience coefficient: {w_experience:.3f} (adds variation)\")\n",
    "print(f\"\\nDecision boundary at probability = 0.5\")\n",
    "print(f\"Transition point (z=0) occurs around age = {-w0/w_age:.1f}\")\n",
    "print(f\"\\nVariable ranges:\")\n",
    "print(f\"  Age: {df['Age'].min():.1f} - {df['Age'].max():.1f} years\")\n",
    "print(f\"  Income: ${df['Income'].min():.0f} - ${df['Income'].max():.0f}\")\n",
    "print(f\"  Experience: {df['Experience'].min():.1f} - {df['Experience'].max():.1f} years\")\n",
    "print(f\"\\nOutcomes:\")\n",
    "print(f\"  No Purchase (y=0): {sum(df['Purchased (y)'] == 0)} people\")\n",
    "print(f\"  Purchase (y=1): {sum(df['Purchased (y)'] == 1)} people\")\n",
    "print(f\"  Probability range: {df['Predicted Probability'].min():.3f} to {df['Predicted Probability'].max():.3f}\")\n",
    "\n",
    "# Show how income/experience affect individual predictions\n",
    "print(f\"\\n=== FEATURE IMPACT EXAMPLES ===\")\n",
    "high_prob_idx = df['Predicted Probability'].idxmax()\n",
    "low_prob_idx = df['Predicted Probability'].idxmin()\n",
    "print(f\"Highest probability ({df.loc[high_prob_idx, 'Predicted Probability']:.3f}):\")\n",
    "print(f\"  Age: {df.loc[high_prob_idx, 'Age']:.1f}, Income: ${df.loc[high_prob_idx, 'Income']:.0f}, Experience: {df.loc[high_prob_idx, 'Experience']:.1f}\")\n",
    "print(f\"Lowest probability ({df.loc[low_prob_idx, 'Predicted Probability']:.3f}):\")\n",
    "print(f\"  Age: {df.loc[low_prob_idx, 'Age']:.1f}, Income: ${df.loc[low_prob_idx, 'Income']:.0f}, Experience: {df.loc[low_prob_idx, 'Experience']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74142f06",
   "metadata": {},
   "source": [
    "## Model Comparison: Feature Selection Impact\n",
    "\n",
    "Now let's compare three different logistic regression models to understand how adding more features affects the predictions:\n",
    "\n",
    "1. **Model 1**: Age only (keeping income and experience constant)\n",
    "2. **Model 2**: Age + Income (keeping experience constant)  \n",
    "3. **Model 3**: Age + Income + Experience (all features)\n",
    "\n",
    "This comparison will help us understand:\n",
    "- How each feature contributes to the prediction\n",
    "- How model complexity affects the decision boundary\n",
    "- The trade-off between simplicity and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552f83b",
   "metadata": {},
   "source": [
    "### Model 1: Age Only - Matrix Formulation\n",
    "\n",
    "In this model, we use **only age** as a predictor while keeping income and experience **constant** at their mean values.\n",
    "\n",
    "**Matrix Setup:**\n",
    "$$X_1 = \\begin{bmatrix}\n",
    "1 & \\text{age}_1 & \\bar{\\text{income}} & \\bar{\\text{experience}} \\\\\n",
    "1 & \\text{age}_2 & \\bar{\\text{income}} & \\bar{\\text{experience}} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & \\text{age}_n & \\bar{\\text{income}} & \\bar{\\text{experience}}\n",
    "\\end{bmatrix}_{n \\times 4}$$\n",
    "\n",
    "**Weight Vector:**\n",
    "$$w_1 = \\begin{bmatrix}\n",
    "w_0 \\\\ w_{\\text{age}} \\\\ 0 \\\\ 0\n",
    "\\end{bmatrix}_{4 \\times 1}$$\n",
    "\n",
    "**Linear Combination:**\n",
    "$$z_1 = X_1 w_1 = w_0 + w_{\\text{age}} \\cdot \\text{age}_i + 0 \\cdot \\bar{\\text{income}} + 0 \\cdot \\bar{\\text{experience}}$$\n",
    "\n",
    "**Simplified:**\n",
    "$$z_1 = w_0 + w_{\\text{age}} \\cdot \\text{age}_i$$\n",
    "\n",
    "**Probability:**\n",
    "$$P(\\text{purchase}_i) = \\sigma(z_1) = \\frac{1}{1 + e^{-(w_0 + w_{\\text{age}} \\cdot \\text{age}_i)}}$$\n",
    "\n",
    "This creates a **pure age-based model** where only age determines the purchase probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1: Age Only (Income and Experience held constant)\n",
    "print(\"=== MODEL 1: AGE ONLY ===\")\n",
    "\n",
    "# Use the same data but fix income and experience at their mean values\n",
    "income_constant = df['Income'].mean()\n",
    "experience_constant = df['Experience'].mean()\n",
    "\n",
    "# Create matrix with age + constant income/experience\n",
    "X1 = np.column_stack((\n",
    "    np.ones(n_samples),                    # bias term\n",
    "    df['Age'],                             # age (variable)\n",
    "    np.full(n_samples, income_constant),   # income (constant)\n",
    "    np.full(n_samples, experience_constant) # experience (constant)\n",
    "))\n",
    "\n",
    "# Simple weights - only age matters, others are zero\n",
    "w1_age = 0.25\n",
    "w1_income = 0.0     # No effect\n",
    "w1_experience = 0.0 # No effect\n",
    "w1_bias = -12\n",
    "\n",
    "w1 = np.array([w1_bias, w1_age, w1_income, w1_experience]).reshape(-1,1)\n",
    "\n",
    "# Calculate probabilities and predictions\n",
    "z1 = X1 @ w1\n",
    "prob1 = sigmoid(z1)\n",
    "pred1 = (prob1.flatten() >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Age coefficient: {w1_age}\")\n",
    "print(f\"Income coefficient: {w1_income} (held constant at ${income_constant:.0f})\")\n",
    "print(f\"Experience coefficient: {w1_experience} (held constant at {experience_constant:.1f} years)\")\n",
    "print(f\"Transition age: {-w1_bias/w1_age:.1f} years\")\n",
    "print(f\"Accuracy: {np.mean(pred1 == df['Purchased (y)']):.3f}\")\n",
    "print(f\"Predicted purchases: {sum(pred1)}/{len(pred1)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2: Age + Income (Experience held constant)\n",
    "print(\"=== MODEL 2: AGE + INCOME ===\")\n",
    "\n",
    "# Create matrix with age + income (variable) + constant experience\n",
    "X2 = np.column_stack((\n",
    "    np.ones(n_samples),                    # bias term\n",
    "    df['Age'],                             # age (variable)\n",
    "    df['Income'],                          # income (variable)\n",
    "    np.full(n_samples, experience_constant) # experience (constant)\n",
    "))\n",
    "\n",
    "# Weights for age and income, experience set to zero\n",
    "w2_age = 0.20       # Slightly reduced since income also contributes\n",
    "w2_income = 0.00003 # Income effect\n",
    "w2_experience = 0.0 # No effect (held constant)\n",
    "w2_bias = -12\n",
    "\n",
    "w2 = np.array([w2_bias, w2_age, w2_income, w2_experience]).reshape(-1,1)\n",
    "\n",
    "# Calculate probabilities and predictions\n",
    "z2 = X2 @ w2\n",
    "prob2 = sigmoid(z2)\n",
    "pred2 = (prob2.flatten() >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Age coefficient: {w2_age}\")\n",
    "print(f\"Income coefficient: {w2_income}\")\n",
    "print(f\"Experience coefficient: {w2_experience} (held constant at {experience_constant:.1f} years)\")\n",
    "print(f\"Base transition age (at mean income): {-w2_bias/w2_age:.1f} years\")\n",
    "print(f\"Income effect: +$10K income ≈ {10000 * w2_income / w2_age:.1f} years younger\")\n",
    "print(f\"Accuracy: {np.mean(pred2 == df['Purchased (y)']):.3f}\")\n",
    "print(f\"Predicted purchases: {sum(pred2)}/{len(pred2)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32f671",
   "metadata": {},
   "source": [
    "### Model 2: Age + Income - Matrix Formulation\n",
    "\n",
    "In this model, we use **age and income** as predictors while keeping experience **constant** at its mean value.\n",
    "\n",
    "**Matrix Setup:**\n",
    "$$X_2 = \\begin{bmatrix}\n",
    "1 & \\text{age}_1 & \\text{income}_1 & \\bar{\\text{experience}} \\\\\n",
    "1 & \\text{age}_2 & \\text{income}_2 & \\bar{\\text{experience}} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & \\text{age}_n & \\text{income}_n & \\bar{\\text{experience}}\n",
    "\\end{bmatrix}_{n \\times 4}$$\n",
    "\n",
    "**Weight Vector:**\n",
    "$$w_2 = \\begin{bmatrix}\n",
    "w_0 \\\\ w_{\\text{age}} \\\\ w_{\\text{income}} \\\\ 0\n",
    "\\end{bmatrix}_{4 \\times 1}$$\n",
    "\n",
    "**Linear Combination:**\n",
    "$$z_2 = X_2 w_2 = w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i + 0 \\cdot \\bar{\\text{experience}}$$\n",
    "\n",
    "**Simplified:**\n",
    "$$z_2 = w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i$$\n",
    "\n",
    "**Probability:**\n",
    "$$P(\\text{purchase}_i) = \\sigma(z_2) = \\frac{1}{1 + e^{-(w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i)}}$$\n",
    "\n",
    "This creates an **age-income interaction model** where both age and individual income levels affect the purchase probability, adding more nuanced predictions than the age-only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 3: Age + Income + Experience (All features)\n",
    "print(\"=== MODEL 3: ALL FEATURES ===\")\n",
    "\n",
    "# Use original matrix with all features\n",
    "X3 = X  # This is the original matrix from earlier\n",
    "\n",
    "# Use original weights (all features contribute)\n",
    "w3_age = 0.18           # Reduced as other features also contribute\n",
    "w3_income = 0.00002     # Income effect\n",
    "w3_experience = 0.04    # Experience effect\n",
    "w3_bias = -12\n",
    "\n",
    "w3 = np.array([w3_bias, w3_age, w3_income, w3_experience]).reshape(-1,1)\n",
    "\n",
    "# Calculate probabilities and predictions\n",
    "z3 = X3 @ w3\n",
    "prob3 = sigmoid(z3)\n",
    "pred3 = (prob3.flatten() >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Age coefficient: {w3_age}\")\n",
    "print(f\"Income coefficient: {w3_income}\")\n",
    "print(f\"Experience coefficient: {w3_experience}\")\n",
    "print(f\"Base transition age: {-w3_bias/w3_age:.1f} years\")\n",
    "print(f\"Income effect: +$10K income ≈ {10000 * w3_income / w3_age:.1f} years younger\")\n",
    "print(f\"Experience effect: +10 years exp ≈ {10 * w3_experience / w3_age:.1f} years younger\")\n",
    "print(f\"Accuracy: {np.mean(pred3 == df['Purchased (y)']):.3f}\")\n",
    "print(f\"Predicted purchases: {sum(pred3)}/{len(pred3)}\")\n",
    "print()\n",
    "\n",
    "# Compare all models\n",
    "print(\"=== MODEL COMPARISON SUMMARY ===\")\n",
    "print(f\"Model 1 (Age only):        Accuracy = {np.mean(pred1 == df['Purchased (y)']):.3f}\")\n",
    "print(f\"Model 2 (Age + Income):    Accuracy = {np.mean(pred2 == df['Purchased (y)']):.3f}\")\n",
    "print(f\"Model 3 (All features):    Accuracy = {np.mean(pred3 == df['Purchased (y)']):.3f}\")\n",
    "print()\n",
    "print(\"Feature importance ranking:\")\n",
    "print(\"1. Age (primary factor in all models)\")\n",
    "print(\"2. Experience (moderate effect)\")\n",
    "print(\"3. Income (smaller effect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f21d2c",
   "metadata": {},
   "source": [
    "### Model 3: All Features - Matrix Formulation\n",
    "\n",
    "In this model, we use **all features** (age, income, and experience) as predictors for the most comprehensive model.\n",
    "\n",
    "**Matrix Setup:**\n",
    "$$X_3 = \\begin{bmatrix}\n",
    "1 & \\text{age}_1 & \\text{income}_1 & \\text{experience}_1 \\\\\n",
    "1 & \\text{age}_2 & \\text{income}_2 & \\text{experience}_2 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & \\text{age}_n & \\text{income}_n & \\text{experience}_n\n",
    "\\end{bmatrix}_{n \\times 4}$$\n",
    "\n",
    "**Weight Vector:**\n",
    "$$w_3 = \\begin{bmatrix}\n",
    "w_0 \\\\ w_{\\text{age}} \\\\ w_{\\text{income}} \\\\ w_{\\text{experience}}\n",
    "\\end{bmatrix}_{4 \\times 1}$$\n",
    "\n",
    "**Linear Combination:**\n",
    "$$z_3 = X_3 w_3 = w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i + w_{\\text{experience}} \\cdot \\text{experience}_i$$\n",
    "\n",
    "**Probability:**\n",
    "$$P(\\text{purchase}_i) = \\sigma(z_3) = \\frac{1}{1 + e^{-(w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i + w_{\\text{experience}} \\cdot \\text{experience}_i)}}$$\n",
    "\n",
    "**Feature Interactions:**\n",
    "This full model captures the **combined effect** of all three features:\n",
    "- **Age**: Primary life stage factor\n",
    "- **Income**: Financial capacity factor  \n",
    "- **Experience**: Professional development factor\n",
    "\n",
    "The model can capture complex interactions where, for example, a younger person with high income and experience might have similar purchase probability as an older person with moderate income and experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43025a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 Visualization: Age Only\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create age range for plotting curves\n",
    "age_range = np.linspace(18, 70, 200)\n",
    "\n",
    "# Model 1: Age only curve\n",
    "X_plot1 = np.column_stack((\n",
    "    np.ones(200),\n",
    "    age_range,\n",
    "    np.full(200, income_constant),\n",
    "    np.full(200, experience_constant)\n",
    "))\n",
    "prob_plot1 = sigmoid(X_plot1 @ w1)\n",
    "\n",
    "plt.plot(age_range, prob_plot1, 'b-', linewidth=3, label='Age Only Model')\n",
    "\n",
    "# Color points based on their ACTUAL PROBABILITY (not outcome)\n",
    "# Red for probabilities < 0.5, Yellow for probabilities >= 0.5\n",
    "colors1 = ['red' if p < 0.5 else 'gold' for p in prob1.flatten()]\n",
    "plt.scatter(df['Age'], prob1.flatten(), c=colors1, alpha=0.8, s=60, \n",
    "           edgecolors='black', linewidth=0.5, zorder=5)\n",
    "\n",
    "# Decision boundary\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, linewidth=2, \n",
    "           label='Decision Boundary (0.5)')\n",
    "\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Predicted Probability', fontsize=12)\n",
    "plt.title('Model 1: Age Only\\n(Income and Experience held constant)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.xlim(18, 70)\n",
    "\n",
    "# Add annotation\n",
    "plt.text(25, 0.8, f'Red: P < 0.5\\nGold: P ≥ 0.5\\n\\nAccuracy: {np.mean(pred1 == df[\"Purchased (y)\"]):.3f}', \n",
    "         fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a37888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 Visualization: Age + Income\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Model 2: Age + Income curve (using mean income for the curve)\n",
    "X_plot2 = np.column_stack((\n",
    "    np.ones(200),\n",
    "    age_range,\n",
    "    np.full(200, df['Income'].mean()),  # Use mean income for the curve\n",
    "    np.full(200, experience_constant)\n",
    "))\n",
    "prob_plot2 = sigmoid(X_plot2 @ w2)\n",
    "\n",
    "plt.plot(age_range, prob_plot2, 'g-', linewidth=3, label='Age + Income Model')\n",
    "\n",
    "# Color points based on their ACTUAL PROBABILITY (not outcome)\n",
    "# Red for probabilities < 0.5, Yellow for probabilities >= 0.5\n",
    "colors2 = ['red' if p < 0.5 else 'gold' for p in prob2.flatten()]\n",
    "plt.scatter(df['Age'], prob2.flatten(), c=colors2, alpha=0.8, s=60, \n",
    "           edgecolors='black', linewidth=0.5, zorder=5)\n",
    "\n",
    "# Decision boundary\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, linewidth=2, \n",
    "           label='Decision Boundary (0.5)')\n",
    "\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Predicted Probability', fontsize=12)\n",
    "plt.title('Model 2: Age + Income\\n(Experience held constant, Income varies by individual)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.xlim(18, 70)\n",
    "\n",
    "# Add annotation\n",
    "plt.text(25, 0.8, f'Red: P < 0.5\\nGold: P ≥ 0.5\\n\\nAccuracy: {np.mean(pred2 == df[\"Purchased (y)\"]):.3f}\\nIncome adds variation', \n",
    "         fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 Visualization: All Features (Age + Income + Experience)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Model 3: All features curve (using mean income and experience for the curve)\n",
    "X_plot3 = np.column_stack((\n",
    "    np.ones(200),\n",
    "    age_range,\n",
    "    np.full(200, df['Income'].mean()),      # Use mean income for the curve\n",
    "    np.full(200, df['Experience'].mean())   # Use mean experience for the curve\n",
    "))\n",
    "prob_plot3 = sigmoid(X_plot3 @ w3)\n",
    "\n",
    "plt.plot(age_range, prob_plot3, 'purple', linewidth=3, label='All Features Model')\n",
    "\n",
    "# Color points based on their ACTUAL PROBABILITY (not outcome)\n",
    "# Red for probabilities < 0.5, Yellow for probabilities >= 0.5\n",
    "colors3 = ['red' if p < 0.5 else 'gold' for p in prob3.flatten()]\n",
    "plt.scatter(df['Age'], prob3.flatten(), c=colors3, alpha=0.8, s=60, \n",
    "           edgecolors='black', linewidth=0.5, zorder=5)\n",
    "\n",
    "# Decision boundary\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, linewidth=2, \n",
    "           label='Decision Boundary (0.5)')\n",
    "\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Predicted Probability', fontsize=12)\n",
    "plt.title('Model 3: All Features (Age + Income + Experience)\\n(Income and Experience vary by individual)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.xlim(18, 70)\n",
    "\n",
    "# Add annotation\n",
    "plt.text(25, 0.8, f'Red: P < 0.5\\nGold: P ≥ 0.5\\n\\nAccuracy: {np.mean(pred3 == df[\"Purchased (y)\"]):.3f}\\nAll features add variation', \n",
    "         fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"plum\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d776894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed comparison table\n",
    "print(\"=== DETAILED MODEL COMPARISON ===\")\n",
    "comparison_data = {\n",
    "    'Model': ['Age Only', 'Age + Income', 'All Features'],\n",
    "    'Features Used': ['Age', 'Age, Income', 'Age, Income, Experience'],\n",
    "    'Age Coefficient': [w1_age, w2_age, w3_age],\n",
    "    'Income Coefficient': [w1_income, w2_income, w3_income],\n",
    "    'Experience Coefficient': [w1_experience, w2_experience, w3_experience],\n",
    "    'Accuracy': [\n",
    "        np.mean(pred1 == df['Purchased (y)']),\n",
    "        np.mean(pred2 == df['Purchased (y)']),\n",
    "        np.mean(pred3 == df['Purchased (y)'])\n",
    "    ],\n",
    "    'Predictions (Yes/Total)': [\n",
    "        f\"{sum(pred1)}/{len(pred1)}\",\n",
    "        f\"{sum(pred2)}/{len(pred2)}\",\n",
    "        f\"{sum(pred3)}/{len(pred3)}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Format the DataFrame for better display\n",
    "comparison_df_display = comparison_df.copy()\n",
    "comparison_df_display['Age Coefficient'] = comparison_df_display['Age Coefficient'].apply(lambda x: f\"{x:.3f}\")\n",
    "comparison_df_display['Income Coefficient'] = comparison_df_display['Income Coefficient'].apply(lambda x: f\"{x:.6f}\")\n",
    "comparison_df_display['Experience Coefficient'] = comparison_df_display['Experience Coefficient'].apply(lambda x: f\"{x:.3f}\")\n",
    "comparison_df_display['Accuracy'] = comparison_df_display['Accuracy'].apply(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "# Display the DataFrame with proper formatting\n",
    "display(comparison_df_display)\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(\"• Model complexity increases from left to right\")\n",
    "print(\"• Age coefficient decreases as more features are added\")\n",
    "print(\"• Additional features provide more nuanced predictions\")\n",
    "print(\"• Color coding: Red points have P < 0.5, Gold points have P ≥ 0.5\")\n",
    "print(\"• Decision boundary at P = 0.5 determines final classification\")\n",
    "\n",
    "# Count points by color for each model\n",
    "red1 = sum(1 for p in prob1.flatten() if p < 0.5)\n",
    "gold1 = sum(1 for p in prob1.flatten() if p >= 0.5)\n",
    "red2 = sum(1 for p in prob2.flatten() if p < 0.5)\n",
    "gold2 = sum(1 for p in prob2.flatten() if p >= 0.5)\n",
    "red3 = sum(1 for p in prob3.flatten() if p < 0.5)\n",
    "gold3 = sum(1 for p in prob3.flatten() if p >= 0.5)\n",
    "\n",
    "print(f\"\\n=== POINT DISTRIBUTION ===\")\n",
    "print(f\"Model 1: {red1} red points (P<0.5), {gold1} gold points (P≥0.5)\")\n",
    "print(f\"Model 2: {red2} red points (P<0.5), {gold2} gold points (P≥0.5)\")\n",
    "print(f\"Model 3: {red3} red points (P<0.5), {gold3} gold points (P≥0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f0d78",
   "metadata": {},
   "source": [
    "# CS103 Project - Logistic Regression\n",
    "\n",
    "The logistic regression model is a statistical model used to predict a **binary outcome**. It is a statistical method used to model the relationship between one or more independent variables and a binary dependent variable — that is, an outcome that has two possible values, such as \"yes/no\" or \"success/failure.\" Unlike linear regression, which predicts a continuous value, logistic regression predicts the probability that a given input belongs to a particular category. It does this by applying the logistic (sigmoid) function to transform the output of a linear combination of the input variables into a value between 0 and 1, representing a probability. The sigmoid function is given by:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "This makes logistic regression widely used in fields like medicine, finance, and machine learning for classification problems such as disease prediction, spam detection, and customer churn analysis. Below is the comparison between linear regression and logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59136297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa90b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)  # feature values\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)  # target values with some noise\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot the data points\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, color='blue', label='Data Points')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(X, y_pred, color='red', linewidth=2, label='Regression Line')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c42a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid helper function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c905066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values for the curve\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y = sigmoid(x)\n",
    "\n",
    "# Generate sample points concentrated near 0 and near 1\n",
    "x_near_0 = np.random.uniform(-10, -5, 15)  # points where sigmoid ~ 0\n",
    "x_middle = np.random.uniform(-2, 2, 10)    # points where sigmoid ~ 0.5\n",
    "x_near_1 = np.random.uniform(5, 10, 15)    # points where sigmoid ~ 1\n",
    "\n",
    "# Compute sigmoid values for these points\n",
    "y_near_0 = sigmoid(x_near_0)\n",
    "y_middle = sigmoid(x_middle)\n",
    "y_near_1 = sigmoid(x_near_1)\n",
    "\n",
    "# Plot the sigmoid curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, color='blue', linewidth=2)\n",
    "\n",
    "# Plot sample points\n",
    "plt.scatter(x_near_0, y_near_0, color='red', zorder=5)\n",
    "plt.scatter(x_near_1, y_near_1, color='orange', zorder=5)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('σ(x)')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c3099",
   "metadata": {},
   "source": [
    " The sigmoid function is primarily used to map any real-valued number into a probability-like value between 0 and 1, making it particularly useful in fields like statistics, machine learning, and neural networks. Its smooth, S-shaped curve ensures that extreme negative inputs produce outputs close to 0, extreme positive inputs produce outputs close to 1, and values near zero produce outputs near 0.5. This property allows the sigmoid function to act as a **squashing function**, compressing unbounded input values into a finite range, which is essential for modeling probabilities and making binary classifications. In logistic regression, the sigmoid function converts the linear combination of input features into a probability that an event belongs to a particular class, while in neural networks, it helps introduce non-linearity, enabling the model to capture complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a7254",
   "metadata": {},
   "source": [
    "## Connection to Linear Algebra\n",
    "\n",
    "1. **Data Representation as Matrices**  \n",
    "   Represent the inputs as a matrix $X$ of size $n \\times m$, where $n$ is the number of samples and $m$ is the number of features.  \n",
    "   \n",
    "   $$ \n",
    "   X = \n",
    "   \\begin{bmatrix}\n",
    "   x_{11} & x_{12} & \\dots & x_{1m} \\\\\n",
    "   x_{21} & x_{22} & \\dots & x_{2m} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   x_{n1} & x_{n2} & \\dots & x_{nm}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   \n",
    "   Weights are represented as a vector $w$ of size $m \\times 1$:\n",
    "   \n",
    "   $$w = \n",
    "   \\begin{bmatrix}\n",
    "   w_1 \\\\\n",
    "   w_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   w_m\n",
    "   \\end{bmatrix}$$\n",
    "\n",
    "2. **Linear Combination in Matrix Form**  \n",
    "   Compute the linear combination of inputs and weights using matrix multiplication:\n",
    "   \n",
    "   $$z = X w$$\n",
    "   \n",
    "   Here, $z$ is an $n \\times 1$ vector where each entry corresponds to a sample:  \n",
    "   \n",
    "   $$z_i = w_1 x_{i1} + w_2 x_{i2} + \\dots + w_m x_{im}$$\n",
    "\n",
    "3. **Gradient Descent Using Matrices**  \n",
    "   The gradient of the binary cross-entropy loss can be computed efficiently using matrices:\n",
    "   \n",
    "   $$\\nabla_w L = \\frac{1}{n} X^T (\\hat{y} - y)$$\n",
    "   \n",
    "   This allows vectorized updates of the weight vector $w$ during gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfb3f9",
   "metadata": {},
   "source": [
    "## Example: Logistic Regression Example: Predicting Purchase of a Premium Online Subscription\n",
    "\n",
    "### Explanation\n",
    "\n",
    "This example simulates a **logistic regression scenario** where we are trying to predict whether a person will **purchase a premium online subscription service** (such as a fitness app, streaming platform, or software tool) based on multiple independent variables:\n",
    "\n",
    "- **Age** (in years)  \n",
    "- **Annual Income** (in dollars)  \n",
    "- **Years of Professional Experience**  \n",
    "\n",
    "Each individual's data is represented as a **row in a matrix $X$**, with an additional column of 1s to account for the bias term. The **weights vector $w$** encodes how strongly each feature influences the likelihood of purchasing the subscription. By computing the **linear combination**:\n",
    "\n",
    "$$z = X w$$\n",
    "\n",
    "we obtain a score for each individual. This score is then passed through the **sigmoid function** to map it into a probability between 0 and 1:\n",
    "\n",
    "$$\\hat{y}_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}}$$\n",
    "\n",
    "Finally, the **binary outcome** $y_i$ is generated based on this probability, simulating whether a person actually purchases the subscription:\n",
    "\n",
    "$$y_i \\sim \\text{Bernoulli}(\\hat{y}_i)$$\n",
    "\n",
    "The dataset can be displayed in a table to show all inputs, the linear combination $z$, predicted probabilities, and the actual binary outcome. This helps visualize how logistic regression transforms raw features into predictions for real-world decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### Matrix Representation\n",
    "\n",
    "Let:\n",
    "\n",
    "$$X =\n",
    "\\begin{bmatrix}\n",
    "1 & \\text{age}_1 & \\text{income}_1 & \\text{experience}_1 \\\\\n",
    "1 & \\text{age}_2 & \\text{income}_2 & \\text{experience}_2 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & \\text{age}_n & \\text{income}_n & \\text{experience}_n\n",
    "\\end{bmatrix}_{n \\times 4}, \n",
    "\\quad\n",
    "w =\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\ w_{\\text{age}} \\\\ w_{\\text{income}} \\\\ w_{\\text{experience}}\n",
    "\\end{bmatrix}_{4 \\times 1}$$\n",
    "\n",
    "1. **Linear combination (score for each individual):**\n",
    "\n",
    "$$z_i = w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i + w_{\\text{experience}} \\cdot \\text{experience}_i$$\n",
    "\n",
    "2. **Predicted probability of purchasing the premium subscription:**\n",
    "\n",
    "$$\\hat{y}_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}}$$\n",
    "\n",
    "3. **Binary outcome (purchase or not):**\n",
    "\n",
    "$$y_i \\sim \\text{Bernoulli}(\\hat{y}_i)$$\n",
    "\n",
    "Where $y_i = 1$ indicates the person purchased the subscription, and $y_i = 0$ indicates they did not.\n",
    "\n",
    "---\n",
    "\n",
    "This Markdown cell now fully explains the example, the real-life context, and the matrix formulation used in logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data to create a clear S-curve pattern with variable income and experience\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "# Create age values that span across the sigmoid transition zone\n",
    "# This ensures we get points both near 0 and near 1, with transition in between\n",
    "age_low = np.random.uniform(20, 35, 15)     # Ages that will likely give low probabilities\n",
    "age_mid = np.random.uniform(35, 50, 20)     # Ages in transition zone\n",
    "age_high = np.random.uniform(50, 65, 15)    # Ages that will likely give high probabilities\n",
    "age = np.concatenate([age_low, age_mid, age_high])\n",
    "\n",
    "# Variable income and experience - but with controlled ranges to maintain S-curve\n",
    "income = np.random.uniform(30000, 80000, n_samples)     # Variable income\n",
    "experience = np.random.uniform(5, 30, n_samples)        # Variable experience\n",
    "\n",
    "# Weights designed to create clear S-curve with age as PRIMARY factor\n",
    "# Income and experience have smaller coefficients to add variation without disrupting S-curve\n",
    "w0 = -12        # bias - keeps the curve centered\n",
    "w_age = 0.25    # STRONG age coefficient (main driver of S-curve)\n",
    "w_income = 0.00002  # small income effect (adds some variation)\n",
    "w_experience = 0.03  # small experience effect (adds some variation)\n",
    "w = np.array([w0, w_age, w_income, w_experience]).reshape(-1,1)\n",
    "\n",
    "# Represent it as a matrix\n",
    "X = np.column_stack((np.ones(n_samples), age, income, experience)) # n x (m+1)\n",
    "\n",
    "# Compute linear combination and predicted probabilities\n",
    "z_matrix = X @ w\n",
    "y_prob = sigmoid(z_matrix)\n",
    "\n",
    "# Generate binary dependent variable using logistic decision rule\n",
    "y = (y_prob.flatten() >= 0.5).astype(int)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Income': income,\n",
    "    'Experience': experience,\n",
    "    'Linear Combination (z)': z_matrix.flatten(),\n",
    "    'Predicted Probability': y_prob.flatten(),\n",
    "    'Purchased (y)': y\n",
    "})\n",
    "\n",
    "# Sort by age for better visualization\n",
    "df = df.sort_values('Age').reset_index(drop=True)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the S-curve plot with variable income and experience\n",
    "# Use mean values for the main curve, but show actual data points with their real values\n",
    "income_mean = df['Income'].mean()\n",
    "experience_mean = df['Experience'].mean()\n",
    "\n",
    "# Create a wide age range to show the full S-curve (using mean income/experience)\n",
    "age_range = np.linspace(15, 70, 300)  # More points for smoother curve\n",
    "X_plot = np.column_stack((\n",
    "    np.ones(300),\n",
    "    age_range,\n",
    "    np.full(300, income_mean),\n",
    "    np.full(300, experience_mean)\n",
    "))\n",
    "\n",
    "# Use the same weights as in data generation\n",
    "w_vector = np.array([w0, w_age, w_income, w_experience]).reshape(-1,1)\n",
    "z_plot = X_plot @ w_vector\n",
    "prob_plot = sigmoid(z_plot)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the smooth S-curve (based on mean income/experience)\n",
    "plt.plot(age_range, prob_plot, color='blue', linewidth=3, \n",
    "         label=f'Sigmoid Curve (Income={income_mean:.0f}, Experience={experience_mean:.1f})')\n",
    "\n",
    "# Plot actual data points with their individual probabilities\n",
    "# Color points by their actual outcome, but position them at their actual probability\n",
    "colors = ['red' if y_val == 0 else 'orange' for y_val in df['Purchased (y)']]\n",
    "scatter = plt.scatter(df['Age'], df['Predicted Probability'], c=colors, s=80, alpha=0.8, \n",
    "                     edgecolors='black', linewidth=0.5, \n",
    "                     label='Actual Data Points (with variable Income/Experience)')\n",
    "\n",
    "# Add a horizontal line at 0.5 to show the decision boundary\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, linewidth=2, \n",
    "           label='Decision Boundary (0.5)')\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('Age (with variable Income and Experience)', fontsize=14)\n",
    "plt.ylabel('Predicted Probability', fontsize=14)\n",
    "plt.title('Logistic Regression: S-Curve with Variable Features', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10, loc='center left')\n",
    "plt.ylim(-0.15, 1.15)\n",
    "plt.xlim(15, 70)\n",
    "\n",
    "# Add text annotations\n",
    "plt.text(25, 0.85, 'S-shaped Sigmoid Curve\\n(Age is primary factor)', fontsize=11, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "plt.text(45, 0.15, f'Transition around age ≈ {-w0/w_age:.0f}\\n(±variation from Income/Experience)', \n",
    "         fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed summary statistics\n",
    "print(f\"=== LOGISTIC REGRESSION SUMMARY ===\")\n",
    "print(f\"Age coefficient: {w_age:.3f} (primary driver)\")\n",
    "print(f\"Income coefficient: {w_income:.6f} (adds variation)\")\n",
    "print(f\"Experience coefficient: {w_experience:.3f} (adds variation)\")\n",
    "print(f\"\\nDecision boundary at probability = 0.5\")\n",
    "print(f\"Transition point (z=0) occurs around age = {-w0/w_age:.1f}\")\n",
    "print(f\"\\nVariable ranges:\")\n",
    "print(f\"  Age: {df['Age'].min():.1f} - {df['Age'].max():.1f} years\")\n",
    "print(f\"  Income: ${df['Income'].min():.0f} - ${df['Income'].max():.0f}\")\n",
    "print(f\"  Experience: {df['Experience'].min():.1f} - {df['Experience'].max():.1f} years\")\n",
    "print(f\"\\nOutcomes:\")\n",
    "print(f\"  No Purchase (y=0): {sum(df['Purchased (y)'] == 0)} people\")\n",
    "print(f\"  Purchase (y=1): {sum(df['Purchased (y)'] == 1)} people\")\n",
    "print(f\"  Probability range: {df['Predicted Probability'].min():.3f} to {df['Predicted Probability'].max():.3f}\")\n",
    "\n",
    "# Show how income/experience affect individual predictions\n",
    "print(f\"\\n=== FEATURE IMPACT EXAMPLES ===\")\n",
    "high_prob_idx = df['Predicted Probability'].idxmax()\n",
    "low_prob_idx = df['Predicted Probability'].idxmin()\n",
    "print(f\"Highest probability ({df.loc[high_prob_idx, 'Predicted Probability']:.3f}):\")\n",
    "print(f\"  Age: {df.loc[high_prob_idx, 'Age']:.1f}, Income: ${df.loc[high_prob_idx, 'Income']:.0f}, Experience: {df.loc[high_prob_idx, 'Experience']:.1f}\")\n",
    "print(f\"Lowest probability ({df.loc[low_prob_idx, 'Predicted Probability']:.3f}):\")\n",
    "print(f\"  Age: {df.loc[low_prob_idx, 'Age']:.1f}, Income: ${df.loc[low_prob_idx, 'Income']:.0f}, Experience: {df.loc[low_prob_idx, 'Experience']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c0d77c",
   "metadata": {},
   "source": [
    "## Model Comparison: Feature Selection Impact\n",
    "\n",
    "Now let's compare three different logistic regression models to understand how adding more features affects the predictions:\n",
    "\n",
    "1. **Model 1**: Age only (keeping income and experience constant)\n",
    "2. **Model 2**: Age + Income (keeping experience constant)  \n",
    "3. **Model 3**: Age + Income + Experience (all features)\n",
    "\n",
    "This comparison will help us understand:\n",
    "- How each feature contributes to the prediction\n",
    "- How model complexity affects the decision boundary\n",
    "- The trade-off between simplicity and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61855361",
   "metadata": {},
   "source": [
    "### Model 1: Age Only - Matrix Formulation\n",
    "\n",
    "In this model, we use **only age** as a predictor while keeping income and experience **constant** at their mean values.\n",
    "\n",
    "**Matrix Setup:**\n",
    "$$X_1 = \\begin{bmatrix}\n",
    "1 & \\text{age}_1 & \\bar{\\text{income}} & \\bar{\\text{experience}} \\\\\n",
    "1 & \\text{age}_2 & \\bar{\\text{income}} & \\bar{\\text{experience}} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & \\text{age}_n & \\bar{\\text{income}} & \\bar{\\text{experience}}\n",
    "\\end{bmatrix}_{n \\times 4}$$\n",
    "\n",
    "**Weight Vector:**\n",
    "$$w_1 = \\begin{bmatrix}\n",
    "w_0 \\\\ w_{\\text{age}} \\\\ 0 \\\\ 0\n",
    "\\end{bmatrix}_{4 \\times 1}$$\n",
    "\n",
    "**Linear Combination:**\n",
    "$$z_1 = X_1 w_1 = w_0 + w_{\\text{age}} \\cdot \\text{age}_i + 0 \\cdot \\bar{\\text{income}} + 0 \\cdot \\bar{\\text{experience}}$$\n",
    "\n",
    "**Simplified:**\n",
    "$$z_1 = w_0 + w_{\\text{age}} \\cdot \\text{age}_i$$\n",
    "\n",
    "**Probability:**\n",
    "$$P(\\text{purchase}_i) = \\sigma(z_1) = \\frac{1}{1 + e^{-(w_0 + w_{\\text{age}} \\cdot \\text{age}_i)}}$$\n",
    "\n",
    "This creates a **pure age-based model** where only age determines the purchase probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90688b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1: Age Only (Income and Experience held constant)\n",
    "print(\"=== MODEL 1: AGE ONLY ===\")\n",
    "\n",
    "# Use the same data but fix income and experience at their mean values\n",
    "income_constant = df['Income'].mean()\n",
    "experience_constant = df['Experience'].mean()\n",
    "\n",
    "# Create matrix with age + constant income/experience\n",
    "X1 = np.column_stack((\n",
    "    np.ones(n_samples),                    # bias term\n",
    "    df['Age'],                             # age (variable)\n",
    "    np.full(n_samples, income_constant),   # income (constant)\n",
    "    np.full(n_samples, experience_constant) # experience (constant)\n",
    "))\n",
    "\n",
    "# Simple weights - only age matters, others are zero\n",
    "w1_age = 0.25\n",
    "w1_income = 0.0     # No effect\n",
    "w1_experience = 0.0 # No effect\n",
    "w1_bias = -12\n",
    "\n",
    "w1 = np.array([w1_bias, w1_age, w1_income, w1_experience]).reshape(-1,1)\n",
    "\n",
    "# Calculate probabilities and predictions\n",
    "z1 = X1 @ w1\n",
    "prob1 = sigmoid(z1)\n",
    "pred1 = (prob1.flatten() >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Age coefficient: {w1_age}\")\n",
    "print(f\"Income coefficient: {w1_income} (held constant at ${income_constant:.0f})\")\n",
    "print(f\"Experience coefficient: {w1_experience} (held constant at {experience_constant:.1f} years)\")\n",
    "print(f\"Transition age: {-w1_bias/w1_age:.1f} years\")\n",
    "print(f\"Accuracy: {np.mean(pred1 == df['Purchased (y)']):.3f}\")\n",
    "print(f\"Predicted purchases: {sum(pred1)}/{len(pred1)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2: Age + Income (Experience held constant)\n",
    "print(\"=== MODEL 2: AGE + INCOME ===\")\n",
    "\n",
    "# Create matrix with age + income (variable) + constant experience\n",
    "X2 = np.column_stack((\n",
    "    np.ones(n_samples),                    # bias term\n",
    "    df['Age'],                             # age (variable)\n",
    "    df['Income'],                          # income (variable)\n",
    "    np.full(n_samples, experience_constant) # experience (constant)\n",
    "))\n",
    "\n",
    "# Weights for age and income, experience set to zero\n",
    "w2_age = 0.20       # Slightly reduced since income also contributes\n",
    "w2_income = 0.00003 # Income effect\n",
    "w2_experience = 0.0 # No effect (held constant)\n",
    "w2_bias = -12\n",
    "\n",
    "w2 = np.array([w2_bias, w2_age, w2_income, w2_experience]).reshape(-1,1)\n",
    "\n",
    "# Calculate probabilities and predictions\n",
    "z2 = X2 @ w2\n",
    "prob2 = sigmoid(z2)\n",
    "pred2 = (prob2.flatten() >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Age coefficient: {w2_age}\")\n",
    "print(f\"Income coefficient: {w2_income}\")\n",
    "print(f\"Experience coefficient: {w2_experience} (held constant at {experience_constant:.1f} years)\")\n",
    "print(f\"Base transition age (at mean income): {-w2_bias/w2_age:.1f} years\")\n",
    "print(f\"Income effect: +$10K income ≈ {10000 * w2_income / w2_age:.1f} years younger\")\n",
    "print(f\"Accuracy: {np.mean(pred2 == df['Purchased (y)']):.3f}\")\n",
    "print(f\"Predicted purchases: {sum(pred2)}/{len(pred2)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b262a8",
   "metadata": {},
   "source": [
    "### Model 2: Age + Income - Matrix Formulation\n",
    "\n",
    "In this model, we use **age and income** as predictors while keeping experience **constant** at its mean value.\n",
    "\n",
    "**Matrix Setup:**\n",
    "$$X_2 = \\begin{bmatrix}\n",
    "1 & \\text{age}_1 & \\text{income}_1 & \\bar{\\text{experience}} \\\\\n",
    "1 & \\text{age}_2 & \\text{income}_2 & \\bar{\\text{experience}} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & \\text{age}_n & \\text{income}_n & \\bar{\\text{experience}}\n",
    "\\end{bmatrix}_{n \\times 4}$$\n",
    "\n",
    "**Weight Vector:**\n",
    "$$w_2 = \\begin{bmatrix}\n",
    "w_0 \\\\ w_{\\text{age}} \\\\ w_{\\text{income}} \\\\ 0\n",
    "\\end{bmatrix}_{4 \\times 1}$$\n",
    "\n",
    "**Linear Combination:**\n",
    "$$z_2 = X_2 w_2 = w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i + 0 \\cdot \\bar{\\text{experience}}$$\n",
    "\n",
    "**Simplified:**\n",
    "$$z_2 = w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i$$\n",
    "\n",
    "**Probability:**\n",
    "$$P(\\text{purchase}_i) = \\sigma(z_2) = \\frac{1}{1 + e^{-(w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i)}}$$\n",
    "\n",
    "This creates an **age-income interaction model** where both age and individual income levels affect the purchase probability, adding more nuanced predictions than the age-only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b94441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 3: Age + Income + Experience (All features)\n",
    "print(\"=== MODEL 3: ALL FEATURES ===\")\n",
    "\n",
    "# Use original matrix with all features\n",
    "X3 = X  # This is the original matrix from earlier\n",
    "\n",
    "# Use original weights (all features contribute)\n",
    "w3_age = 0.18           # Reduced as other features also contribute\n",
    "w3_income = 0.00002     # Income effect\n",
    "w3_experience = 0.04    # Experience effect\n",
    "w3_bias = -12\n",
    "\n",
    "w3 = np.array([w3_bias, w3_age, w3_income, w3_experience]).reshape(-1,1)\n",
    "\n",
    "# Calculate probabilities and predictions\n",
    "z3 = X3 @ w3\n",
    "prob3 = sigmoid(z3)\n",
    "pred3 = (prob3.flatten() >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Age coefficient: {w3_age}\")\n",
    "print(f\"Income coefficient: {w3_income}\")\n",
    "print(f\"Experience coefficient: {w3_experience}\")\n",
    "print(f\"Base transition age: {-w3_bias/w3_age:.1f} years\")\n",
    "print(f\"Income effect: +$10K income ≈ {10000 * w3_income / w3_age:.1f} years younger\")\n",
    "print(f\"Experience effect: +10 years exp ≈ {10 * w3_experience / w3_age:.1f} years younger\")\n",
    "print(f\"Accuracy: {np.mean(pred3 == df['Purchased (y)']):.3f}\")\n",
    "print(f\"Predicted purchases: {sum(pred3)}/{len(pred3)}\")\n",
    "print()\n",
    "\n",
    "# Compare all models\n",
    "print(\"=== MODEL COMPARISON SUMMARY ===\")\n",
    "print(f\"Model 1 (Age only):        Accuracy = {np.mean(pred1 == df['Purchased (y)']):.3f}\")\n",
    "print(f\"Model 2 (Age + Income):    Accuracy = {np.mean(pred2 == df['Purchased (y)']):.3f}\")\n",
    "print(f\"Model 3 (All features):    Accuracy = {np.mean(pred3 == df['Purchased (y)']):.3f}\")\n",
    "print()\n",
    "print(\"Feature importance ranking:\")\n",
    "print(\"1. Age (primary factor in all models)\")\n",
    "print(\"2. Experience (moderate effect)\")\n",
    "print(\"3. Income (smaller effect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c435294",
   "metadata": {},
   "source": [
    "### Model 3: All Features - Matrix Formulation\n",
    "\n",
    "In this model, we use **all features** (age, income, and experience) as predictors for the most comprehensive model.\n",
    "\n",
    "**Matrix Setup:**\n",
    "$$X_3 = \\begin{bmatrix}\n",
    "1 & \\text{age}_1 & \\text{income}_1 & \\text{experience}_1 \\\\\n",
    "1 & \\text{age}_2 & \\text{income}_2 & \\text{experience}_2 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & \\text{age}_n & \\text{income}_n & \\text{experience}_n\n",
    "\\end{bmatrix}_{n \\times 4}$$\n",
    "\n",
    "**Weight Vector:**\n",
    "$$w_3 = \\begin{bmatrix}\n",
    "w_0 \\\\ w_{\\text{age}} \\\\ w_{\\text{income}} \\\\ w_{\\text{experience}}\n",
    "\\end{bmatrix}_{4 \\times 1}$$\n",
    "\n",
    "**Linear Combination:**\n",
    "$$z_3 = X_3 w_3 = w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i + w_{\\text{experience}} \\cdot \\text{experience}_i$$\n",
    "\n",
    "**Probability:**\n",
    "$$P(\\text{purchase}_i) = \\sigma(z_3) = \\frac{1}{1 + e^{-(w_0 + w_{\\text{age}} \\cdot \\text{age}_i + w_{\\text{income}} \\cdot \\text{income}_i + w_{\\text{experience}} \\cdot \\text{experience}_i)}}$$\n",
    "\n",
    "**Feature Interactions:**\n",
    "This full model captures the **combined effect** of all three features:\n",
    "- **Age**: Primary life stage factor\n",
    "- **Income**: Financial capacity factor  \n",
    "- **Experience**: Professional development factor\n",
    "\n",
    "The model can capture complex interactions where, for example, a younger person with high income and experience might have similar purchase probability as an older person with moderate income and experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a13c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 Visualization: Age Only\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create age range for plotting curves\n",
    "age_range = np.linspace(18, 70, 200)\n",
    "\n",
    "# Model 1: Age only curve\n",
    "X_plot1 = np.column_stack((\n",
    "    np.ones(200),\n",
    "    age_range,\n",
    "    np.full(200, income_constant),\n",
    "    np.full(200, experience_constant)\n",
    "))\n",
    "prob_plot1 = sigmoid(X_plot1 @ w1)\n",
    "\n",
    "plt.plot(age_range, prob_plot1, 'b-', linewidth=3, label='Age Only Model')\n",
    "\n",
    "# Color points based on their ACTUAL PROBABILITY (not outcome)\n",
    "# Red for probabilities < 0.5, Yellow for probabilities >= 0.5\n",
    "colors1 = ['red' if p < 0.5 else 'gold' for p in prob1.flatten()]\n",
    "plt.scatter(df['Age'], prob1.flatten(), c=colors1, alpha=0.8, s=60, \n",
    "           edgecolors='black', linewidth=0.5, zorder=5)\n",
    "\n",
    "# Decision boundary\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, linewidth=2, \n",
    "           label='Decision Boundary (0.5)')\n",
    "\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Predicted Probability', fontsize=12)\n",
    "plt.title('Model 1: Age Only\\n(Income and Experience held constant)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.xlim(18, 70)\n",
    "\n",
    "# Add annotation\n",
    "plt.text(25, 0.8, f'Red: P < 0.5\\nGold: P ≥ 0.5\\n\\nAccuracy: {np.mean(pred1 == df[\"Purchased (y)\"]):.3f}', \n",
    "         fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 Visualization: Age + Income\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Model 2: Age + Income curve (using mean income for the curve)\n",
    "X_plot2 = np.column_stack((\n",
    "    np.ones(200),\n",
    "    age_range,\n",
    "    np.full(200, df['Income'].mean()),  # Use mean income for the curve\n",
    "    np.full(200, experience_constant)\n",
    "))\n",
    "prob_plot2 = sigmoid(X_plot2 @ w2)\n",
    "\n",
    "plt.plot(age_range, prob_plot2, 'g-', linewidth=3, label='Age + Income Model')\n",
    "\n",
    "# Color points based on their ACTUAL PROBABILITY (not outcome)\n",
    "# Red for probabilities < 0.5, Yellow for probabilities >= 0.5\n",
    "colors2 = ['red' if p < 0.5 else 'gold' for p in prob2.flatten()]\n",
    "plt.scatter(df['Age'], prob2.flatten(), c=colors2, alpha=0.8, s=60, \n",
    "           edgecolors='black', linewidth=0.5, zorder=5)\n",
    "\n",
    "# Decision boundary\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, linewidth=2, \n",
    "           label='Decision Boundary (0.5)')\n",
    "\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Predicted Probability', fontsize=12)\n",
    "plt.title('Model 2: Age + Income\\n(Experience held constant, Income varies by individual)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.xlim(18, 70)\n",
    "\n",
    "# Add annotation\n",
    "plt.text(25, 0.8, f'Red: P < 0.5\\nGold: P ≥ 0.5\\n\\nAccuracy: {np.mean(pred2 == df[\"Purchased (y)\"]):.3f}\\nIncome adds variation', \n",
    "         fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 Visualization: All Features (Age + Income + Experience)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Model 3: All features curve (using mean income and experience for the curve)\n",
    "X_plot3 = np.column_stack((\n",
    "    np.ones(200),\n",
    "    age_range,\n",
    "    np.full(200, df['Income'].mean()),      # Use mean income for the curve\n",
    "    np.full(200, df['Experience'].mean())   # Use mean experience for the curve\n",
    "))\n",
    "prob_plot3 = sigmoid(X_plot3 @ w3)\n",
    "\n",
    "plt.plot(age_range, prob_plot3, 'purple', linewidth=3, label='All Features Model')\n",
    "\n",
    "# Color points based on their ACTUAL PROBABILITY (not outcome)\n",
    "# Red for probabilities < 0.5, Yellow for probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
